{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8CZZBqQ+03f0dYb5lopvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWKim-postech/PlayGround/blob/main/DropOut_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ROjqZ6utIk"
      },
      "source": [
        "#DropOut during the back-propagation\r\n",
        "\r\n",
        "Suddenly, I wonder that what does happen if I use dropout during the back propagation? (On loss function)\r\n",
        "\r\n",
        "So I built simple FC model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Q6dqa_xy-W"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchvision import datasets, transforms \r\n",
        "import argparse\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH7jhQN94743"
      },
      "source": [
        "parser = argparse.ArgumentParser()\r\n",
        "\r\n",
        "parser.add_argument(\"--batchsize\", type = int, default = 32)\r\n",
        "parser.add_argument(\"--epochs\", type = int, default = 10)\r\n",
        "parser.add_argument(\"--betas\", type = float, default = (0.9, 0.999))\r\n",
        "parser.add_argument(\"--lr\", type = float, default = 0.001)\r\n",
        "parser.add_argument(\"--img\", type = int, default = 28*28)\r\n",
        "\r\n",
        "opt = parser.parse_args(args=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr_MDpJ74MdQ"
      },
      "source": [
        "train_dataset = datasets.MNIST(\"root = ../MNIST\", train = True, download=True, transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\r\n",
        "test_dataset = datasets.MNIST(\"root = ../MNIST\", train = False, download=True, transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=opt.batchsize, shuffle=True)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=opt.batchsize, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIZdy9WHBQXl"
      },
      "source": [
        "\r\n",
        "\r\n",
        "1.   2 Dropout layers in forward()\r\n",
        "2.   Single dropout layer in loss()\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Htesvop43s5"
      },
      "source": [
        "device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "class FC(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(FC, self).__init__()\r\n",
        "    self.fc1 = nn.Linear(opt.img, 256)\r\n",
        "    self.fc2 = nn.Linear(256, 128)\r\n",
        "    self.fc3 = nn.Linear(128, 10)\r\n",
        "\r\n",
        "    self.BN1 = nn.BatchNorm1d(256)\r\n",
        "    self.BN2 = nn.BatchNorm1d(128)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = x.view(-1, opt.img)\r\n",
        "\r\n",
        "    x = self.fc1(x)\r\n",
        "    x = self.BN1(x)\r\n",
        "    x = F.relu(x)\r\n",
        "    #x = F.dropout(x, p = 0.5)\r\n",
        "\r\n",
        "    x = self.fc2(x)\r\n",
        "    x = self.BN2(x)\r\n",
        "    x = F.relu(x)\r\n",
        "    #x = F.dropout(x, p = 0.5)\r\n",
        "\r\n",
        "    x = self.fc3(x)\r\n",
        "    x = F.log_softmax(x)\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QilWaI-967im"
      },
      "source": [
        "model = FC().to(device)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = opt.lr, betas = opt.betas)\r\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNL3VCIe7NbN"
      },
      "source": [
        "def eval(model, test_loader):\r\n",
        "  model.eval()\r\n",
        "  acc = 0\r\n",
        "  loss = 0\r\n",
        "  with torch.no_grad():\r\n",
        "   for img, label in test_loader:\r\n",
        "      img = img.to(device)\r\n",
        "      label = label.to(device)\r\n",
        "      output = model(img)\r\n",
        "      loss += criterion(output, label)\r\n",
        "      pred = output.max(1, keepdim = True)[1]\r\n",
        "      acc += pred.eq(label.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "  return loss/len(test_loader.dataset), 100*acc/len(test_loader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2SOKIBvOqa"
      },
      "source": [
        "#I add DropOut layer on the loss function. \r\n",
        "\r\n",
        "And if you run this code and compare the result with 2 DropOut layers on forward(), you will find out that this model shows much more powerful performance.\r\n",
        "\r\n",
        "So I felt like \"Wow I find out something new!\" But after running this model without any DropOut-layer, I realize that No DropOut model is the best.\r\n",
        "\r\n",
        "It was just happening and meaningless work.. But one day, I'm sure that my curiosity will find something cool.\r\n",
        "\r\n",
        "##And I think it can be useful on the more complex dataset which makes DropOut technique work well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-opI-gc8rD8",
        "outputId": "8a45c7ef-ec2b-4caa-d7af-1bd7ed6f8052"
      },
      "source": [
        "start = time.time()\r\n",
        "for epoch in range(1, opt.epochs+1):\r\n",
        "  model.train()\r\n",
        "  for batch_index, (img, label) in enumerate(train_loader):\r\n",
        "    img = img.to(device)\r\n",
        "    label = label.to(device)\r\n",
        "    output = model(img)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss = criterion(output, label)\r\n",
        "    loss = F.dropout(loss, p = 0.3)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    if batch_index % 200 == 0 :\r\n",
        "      print(\"[Epochs : %d] [batch_index = %d] [loss = %f]\"%(epoch, batch_index, loss.item()))\r\n",
        "\r\n",
        "  Evaluate_loss, acc = eval(model, test_loader)\r\n",
        "  print(\"Evaluate Loss =\", Evaluate_loss.item(), \"\\tAccuracy(%) =\", acc)\r\n",
        "  \r\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epochs : 1] [batch_index = 0] [loss = 2.569268]\n",
            "[Epochs : 1] [batch_index = 200] [loss = 0.486209]\n",
            "[Epochs : 1] [batch_index = 400] [loss = 0.344182]\n",
            "[Epochs : 1] [batch_index = 600] [loss = 0.068658]\n",
            "[Epochs : 1] [batch_index = 800] [loss = 0.453863]\n",
            "[Epochs : 1] [batch_index = 1000] [loss = 0.282217]\n",
            "[Epochs : 1] [batch_index = 1200] [loss = 0.127292]\n",
            "[Epochs : 1] [batch_index = 1400] [loss = 0.032784]\n",
            "[Epochs : 1] [batch_index = 1600] [loss = 0.250965]\n",
            "[Epochs : 1] [batch_index = 1800] [loss = 0.060236]\n",
            "Evaluate Loss = 0.002982230857014656 \tAccuracy(%) = 97.02\n",
            "[Epochs : 2] [batch_index = 0] [loss = 0.273739]\n",
            "[Epochs : 2] [batch_index = 200] [loss = 0.058181]\n",
            "[Epochs : 2] [batch_index = 400] [loss = 0.070183]\n",
            "[Epochs : 2] [batch_index = 600] [loss = 0.027527]\n",
            "[Epochs : 2] [batch_index = 800] [loss = 0.044461]\n",
            "[Epochs : 2] [batch_index = 1000] [loss = 0.155629]\n",
            "[Epochs : 2] [batch_index = 1200] [loss = 0.015186]\n",
            "[Epochs : 2] [batch_index = 1400] [loss = 0.029316]\n",
            "[Epochs : 2] [batch_index = 1600] [loss = 0.012143]\n",
            "[Epochs : 2] [batch_index = 1800] [loss = 0.156824]\n",
            "Evaluate Loss = 0.0023414113093167543 \tAccuracy(%) = 97.64\n",
            "[Epochs : 3] [batch_index = 0] [loss = 0.045981]\n",
            "[Epochs : 3] [batch_index = 200] [loss = 0.030397]\n",
            "[Epochs : 3] [batch_index = 400] [loss = 0.033252]\n",
            "[Epochs : 3] [batch_index = 600] [loss = 0.005494]\n",
            "[Epochs : 3] [batch_index = 800] [loss = 0.062954]\n",
            "[Epochs : 3] [batch_index = 1000] [loss = 0.026457]\n",
            "[Epochs : 3] [batch_index = 1200] [loss = 0.059599]\n",
            "[Epochs : 3] [batch_index = 1400] [loss = 0.031283]\n",
            "[Epochs : 3] [batch_index = 1600] [loss = 0.004839]\n",
            "[Epochs : 3] [batch_index = 1800] [loss = 0.026577]\n",
            "Evaluate Loss = 0.0027674410957843065 \tAccuracy(%) = 97.43\n",
            "[Epochs : 4] [batch_index = 0] [loss = 0.014818]\n",
            "[Epochs : 4] [batch_index = 200] [loss = 0.009456]\n",
            "[Epochs : 4] [batch_index = 400] [loss = 0.009612]\n",
            "[Epochs : 4] [batch_index = 600] [loss = 0.011852]\n",
            "[Epochs : 4] [batch_index = 800] [loss = 0.018491]\n",
            "[Epochs : 4] [batch_index = 1000] [loss = 0.014422]\n",
            "[Epochs : 4] [batch_index = 1200] [loss = 0.100622]\n",
            "[Epochs : 4] [batch_index = 1400] [loss = 0.118516]\n",
            "[Epochs : 4] [batch_index = 1600] [loss = 0.197681]\n",
            "[Epochs : 4] [batch_index = 1800] [loss = 0.032317]\n",
            "Evaluate Loss = 0.002187105594202876 \tAccuracy(%) = 97.83\n",
            "[Epochs : 5] [batch_index = 0] [loss = 0.004537]\n",
            "[Epochs : 5] [batch_index = 200] [loss = 0.005968]\n",
            "[Epochs : 5] [batch_index = 400] [loss = 0.016215]\n",
            "[Epochs : 5] [batch_index = 600] [loss = 0.037390]\n",
            "[Epochs : 5] [batch_index = 800] [loss = 0.087175]\n",
            "[Epochs : 5] [batch_index = 1000] [loss = 0.035870]\n",
            "[Epochs : 5] [batch_index = 1200] [loss = 0.084677]\n",
            "[Epochs : 5] [batch_index = 1400] [loss = 0.003340]\n",
            "[Epochs : 5] [batch_index = 1600] [loss = 0.033984]\n",
            "[Epochs : 5] [batch_index = 1800] [loss = 0.212076]\n",
            "Evaluate Loss = 0.002173881744965911 \tAccuracy(%) = 97.91\n",
            "[Epochs : 6] [batch_index = 0] [loss = 0.064026]\n",
            "[Epochs : 6] [batch_index = 200] [loss = 0.002360]\n",
            "[Epochs : 6] [batch_index = 400] [loss = 0.008632]\n",
            "[Epochs : 6] [batch_index = 600] [loss = 0.177098]\n",
            "[Epochs : 6] [batch_index = 800] [loss = 0.002887]\n",
            "[Epochs : 6] [batch_index = 1000] [loss = 0.001421]\n",
            "[Epochs : 6] [batch_index = 1200] [loss = 0.012238]\n",
            "[Epochs : 6] [batch_index = 1400] [loss = 0.020633]\n",
            "[Epochs : 6] [batch_index = 1600] [loss = 0.091650]\n",
            "[Epochs : 6] [batch_index = 1800] [loss = 0.032321]\n",
            "Evaluate Loss = 0.002035017590969801 \tAccuracy(%) = 98.05\n",
            "[Epochs : 7] [batch_index = 0] [loss = 0.008170]\n",
            "[Epochs : 7] [batch_index = 200] [loss = 0.000000]\n",
            "[Epochs : 7] [batch_index = 400] [loss = 0.000000]\n",
            "[Epochs : 7] [batch_index = 600] [loss = 0.097374]\n",
            "[Epochs : 7] [batch_index = 800] [loss = 0.322068]\n",
            "[Epochs : 7] [batch_index = 1000] [loss = 0.000000]\n",
            "[Epochs : 7] [batch_index = 1200] [loss = 0.000000]\n",
            "[Epochs : 7] [batch_index = 1400] [loss = 0.000000]\n",
            "[Epochs : 7] [batch_index = 1600] [loss = 0.000000]\n",
            "[Epochs : 7] [batch_index = 1800] [loss = 0.019441]\n",
            "Evaluate Loss = 0.002346478635445237 \tAccuracy(%) = 97.66\n",
            "[Epochs : 8] [batch_index = 0] [loss = 0.295110]\n",
            "[Epochs : 8] [batch_index = 200] [loss = 0.087316]\n",
            "[Epochs : 8] [batch_index = 400] [loss = 0.000000]\n",
            "[Epochs : 8] [batch_index = 600] [loss = 0.028522]\n",
            "[Epochs : 8] [batch_index = 800] [loss = 0.000000]\n",
            "[Epochs : 8] [batch_index = 1000] [loss = 0.000000]\n",
            "[Epochs : 8] [batch_index = 1200] [loss = 0.424192]\n",
            "[Epochs : 8] [batch_index = 1400] [loss = 0.049628]\n",
            "[Epochs : 8] [batch_index = 1600] [loss = 0.000000]\n",
            "[Epochs : 8] [batch_index = 1800] [loss = 0.038881]\n",
            "Evaluate Loss = 0.0021032411605119705 \tAccuracy(%) = 97.98\n",
            "[Epochs : 9] [batch_index = 0] [loss = 0.013737]\n",
            "[Epochs : 9] [batch_index = 200] [loss = 0.114268]\n",
            "[Epochs : 9] [batch_index = 400] [loss = 0.189385]\n",
            "[Epochs : 9] [batch_index = 600] [loss = 0.103987]\n",
            "[Epochs : 9] [batch_index = 800] [loss = 0.468230]\n",
            "[Epochs : 9] [batch_index = 1000] [loss = 0.865574]\n",
            "[Epochs : 9] [batch_index = 1200] [loss = 0.060793]\n",
            "[Epochs : 9] [batch_index = 1400] [loss = 0.009197]\n",
            "[Epochs : 9] [batch_index = 1600] [loss = 0.000000]\n",
            "[Epochs : 9] [batch_index = 1800] [loss = 0.003518]\n",
            "Evaluate Loss = 0.0020006673876196146 \tAccuracy(%) = 97.99\n",
            "[Epochs : 10] [batch_index = 0] [loss = 0.000000]\n",
            "[Epochs : 10] [batch_index = 200] [loss = 0.000000]\n",
            "[Epochs : 10] [batch_index = 400] [loss = 0.166956]\n",
            "[Epochs : 10] [batch_index = 600] [loss = 0.000000]\n",
            "[Epochs : 10] [batch_index = 800] [loss = 0.000000]\n",
            "[Epochs : 10] [batch_index = 1000] [loss = 0.000000]\n",
            "[Epochs : 10] [batch_index = 1200] [loss = 0.160185]\n",
            "[Epochs : 10] [batch_index = 1400] [loss = 0.001030]\n",
            "[Epochs : 10] [batch_index = 1600] [loss = 0.058246]\n",
            "[Epochs : 10] [batch_index = 1800] [loss = 0.000000]\n",
            "Evaluate Loss = 0.0021104745101183653 \tAccuracy(%) = 98.01\n",
            "276.1175501346588\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}